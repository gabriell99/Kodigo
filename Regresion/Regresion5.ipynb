{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "719d17ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters (XGBoost):\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1.0}\n",
      "\n",
      "Model Performance Metrics (XGBoost):\n",
      "Mean Squared Error (MSE): 624373057.1668091\n",
      "Root Mean Squared Error (RMSE): 24987.457997299545\n",
      "Mean Absolute Error (MAE): 20608.76171875\n",
      "R-squared (R^2): 0.9744371175765991\n",
      "\n",
      "Predicted Price for the new house (2000 sq ft, 3 bedrooms, 10 years old): 342338.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nelso\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Step 1: Create a dataset with house features and prices\n",
    "data = {\n",
    "    \"Size (sq ft)\": [1200, 1500, 1800, 2500, 1100, 1400, 1600, 2000, 2200, 3000,\n",
    "                     800, 1000, 2700, 1900, 2100, 1700, 2300, 3200, 2800, 3500],\n",
    "    \"Bedrooms\": [2, 3, 3, 4, 2, 3, 3, 4, 4, 5,\n",
    "                 1, 2, 5, 3, 4, 3, 4, 5, 4, 5],\n",
    "    \"Age (years)\": [10, 15, 20, 5, 25, 30, 15, 10, 8, 4,\n",
    "                    50, 45, 3, 12, 7, 20, 6, 2, 9, 1],\n",
    "    \"Price (USD)\": [200000, 250000, 300000, 400000, 150000, 220000, 270000, 350000, 370000, 500000,\n",
    "                    100000, 120000, 480000, 320000, 400000, 290000, 450000, 600000, 500000, 700000]\n",
    "}\n",
    "\n",
    "# Convert the data dictionary into a Pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 2: Define features (X) and target variable (y)\n",
    "X = df[[\"Size (sq ft)\", \"Bedrooms\", \"Age (years)\"]]  # Features\n",
    "y = df[\"Price (USD)\"]  # Target variable\n",
    "\n",
    "# Step 3: Normalize features using Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Split the normalized data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Train an XGBoost Regressor\n",
    "# Perform GridSearchCV to find the best hyperparameters\n",
    "param_grid_xgb = {\n",
    "    \"n_estimators\": [50, 100, 200],  # Number of boosting rounds\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],  # Step size shrinkage\n",
    "    \"max_depth\": [3, 5, 7],  # Maximum depth of trees\n",
    "    \"min_child_weight\": [1, 3, 5],  # Minimum sum of weights of all observations needed in a child\n",
    "    \"subsample\": [0.8, 1.0],  # Fraction of samples used for training\n",
    "    \"colsample_bytree\": [0.8, 1.0]  # Fraction of features used for training each tree\n",
    "}\n",
    "\n",
    "grid_search_xgb = GridSearchCV(\n",
    "    estimator=XGBRegressor(objective=\"reg:squarederror\", random_state=42),\n",
    "    param_grid=param_grid_xgb,\n",
    "    scoring=\"neg_mean_squared_error\",  # Use MSE as the scoring metric\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV on the training data\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the best model and parameters\n",
    "best_xgb_model = grid_search_xgb.best_estimator_\n",
    "best_xgb_params = grid_search_xgb.best_params_\n",
    "\n",
    "# Step 5: Evaluate the best XGBoost model on the test set\n",
    "y_pred_xgb = best_xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "rmse_xgb = np.sqrt(mse_xgb)\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "# Display the best hyperparameters and model performance\n",
    "print(\"Best Hyperparameters (XGBoost):\")\n",
    "print(best_xgb_params)\n",
    "print(\"\\nModel Performance Metrics (XGBoost):\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_xgb}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_xgb}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_xgb}\")\n",
    "print(f\"R-squared (R^2): {r2_xgb}\")\n",
    "\n",
    "# Step 6: Predict the price for a new house using the XGBoost model\n",
    "new_house = [[2000, 3, 10]]  # Features of the new house\n",
    "new_house_normalized = scaler.transform(new_house)  # Normalize the new house features\n",
    "predicted_price_xgb = best_xgb_model.predict(new_house_normalized)\n",
    "print(\"\\nPredicted Price for the new house (2000 sq ft, 3 bedrooms, 10 years old):\", predicted_price_xgb[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
